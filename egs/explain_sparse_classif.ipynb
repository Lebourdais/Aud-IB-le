{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc3dcb5",
   "metadata": {},
   "source": [
    "# Extracting explanations from a sparse audio classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f843eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audIBle.nn.autoencoders import SpecAE\n",
    "from audIBle.nn.sparse_classif import SparseClassifier\n",
    "from audIBle.data.datasets import UrbanSound8k\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f369da",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ids = [\"001\", \"002\", \"003\"]\n",
    "seed=42\n",
    "\n",
    "id_idx = 1\n",
    "exp_name = f\"{config_ids[id_idx]}_sparse_classif_urbasound8k_{seed}\"\n",
    "exp_root = os.path.join(os.environ[\"EXP_ROOT\"], \"train/SAE/ae_debug/\",exp_name)\n",
    "\n",
    "with open(os.path.join(exp_root, \"config.json\"), 'r') as fh:\n",
    "    cfg = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6932883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "autoencoder = SpecAE(**cfg[\"model\"][\"autoencoder\"])\n",
    "ae_ckpt_path = cfg[\"model\"][\"ae_ckpt_path\"]\n",
    "if ae_ckpt_path is not None:\n",
    "    ae_ckpt = torch.load(ae_ckpt_path, map_location=device, weights_only=True)\n",
    "    autoencoder.load_state_dict(ae_ckpt)\n",
    "\n",
    "# prepare the sparse classifier\n",
    "classif_params = cfg[\"model\"][\"classifier\"]\n",
    "classif_params[\"autoencoder\"] = autoencoder\n",
    "model = SparseClassifier(**classif_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d2f5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier checkpoint\n",
    "classif_ckpt_path = os.path.join(exp_root,\"best_model.pth\")\n",
    "ckpt = torch.load(classif_ckpt_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914b6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/lium/corpus/vrac/tmario/sed/urbansound8k/urbansound8k\"\n",
    "fold_test = 10\n",
    "dataset = UrbanSound8k(csv_path=os.path.join(data_root, \"metadata/UrbanSound8K.csv\"),\n",
    "                           audio_dir=os.path.join(data_root, \"audio\"),\n",
    "                           sample_rate=cfg[\"sample_rate\"],\n",
    "                           folds_to_use=[10],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance(model, tau, input_data, label, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # extract all of the representations\n",
    "    all_rep = model.forward_return_all(input_data.to(device))\n",
    "    # pooled sparse representation (used to make the decision)\n",
    "    Z = all_rep[\"sparse_latent_pooled\"]\n",
    "    # weights of the linear classification head\n",
    "    W_c = model.classif_head.weight\n",
    "    #weights of the SAE decoder (dictionary)\n",
    "    W_sae_d = model.sae.decoder.weight\n",
    "\n",
    "    n_classes = W_c.shape[0]\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        # select the weights of the classification layer according to class c\n",
    "        w_class = W_c[c,:]\n",
    "        r_c = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7b87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1024])\n",
      "torch.Size([256, 1024])\n",
      "spec: torch.Size([1, 1, 513, 345])\n",
      "spec_hat: torch.Size([1, 1, 513, 345])\n",
      "ae_hidden: torch.Size([1, 345, 256])\n",
      "ae_hidden_hat: torch.Size([1, 345, 256])\n",
      "sparse_latent: torch.Size([1, 345, 1024])\n",
      "sparse_latent_pooled: torch.Size([1, 1024])\n",
      "attention_weights: torch.Size([1, 345, 1])\n",
      "logits: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "W_c = model.classif_head.weight\n",
    "W_sae_d = model.sae.decoder.weight\n",
    "\n",
    "print(W_c.shape)\n",
    "print(W_sae_d.shape)\n",
    "for i_data in range(len(dataset)):\n",
    "    wav, label = dataset[i_data]\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    all_rep = model.forward_return_all(wav.to(device))\n",
    "\n",
    "    for k,v in all_rep.items():\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8680f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_maps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
